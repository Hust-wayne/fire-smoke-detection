{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import sys\n",
    "from functools import partial\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import lr_scheduler\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import PIL\n",
    "import imgaug as ia\n",
    "import imgaug.augmenters as iaa\n",
    "from imgaug.augmentables.segmaps import SegmentationMapsOnImage\n",
    "\n",
    "from dataset import *\n",
    "from resnet import *\n",
    "from slice_net import *\n",
    "from classifier import *\n",
    "from sampler import BalancedStatisticSampler\n",
    "from visualize import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_NAME = 'slicenet'\n",
    "TRAIN_ID = '09'\n",
    "EPOCH = 186\n",
    "\n",
    "SHOW_COUNT = 50\n",
    "\n",
    "SCORE_THRESHOLD = 0.5\n",
    "MASK_RESIZE_RATIO = 32\n",
    "\n",
    "# data consts\n",
    "ROOT_PATH = '/home/xd/data/fire'\n",
    "NUM_CLASSES = 2 # fg + 1(bg)\n",
    "INPUT_SIZE = (640, 480)\n",
    "BATCH_SIZE = 64\n",
    "NUM_WORKERS = 16\n",
    "\n",
    "# trainer consts\n",
    "DEVICE = 'cuda'\n",
    "\n",
    "# model consts\n",
    "ATTENTION = 'non_local'\n",
    "R = 16\n",
    "K = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_seq = iaa.Sequential([\n",
    "    iaa.Resize({'height':INPUT_SIZE[1], 'width':INPUT_SIZE[0]})\n",
    "])\n",
    "\n",
    "tensor_trans = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(\n",
    "        mean=[0.485, 0.456, 0.406],\n",
    "        std=[0.229, 0.224, 0.225]\n",
    "    )\n",
    "])\n",
    "\n",
    "mask_trans = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((INPUT_SIZE[1]//MASK_RESIZE_RATIO, INPUT_SIZE[0]//MASK_RESIZE_RATIO), PIL.Image.NEAREST),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def trans(\n",
    "    img, mask,\n",
    "    iaa_seq=None,\n",
    "    vision_trans=None,\n",
    "    mask_trans=None\n",
    "):\n",
    "    mask_map = SegmentationMapsOnImage(mask, shape=img.shape)\n",
    "    img_aug, mask_map_aug = iaa_seq(image=img, segmentation_maps=mask_map)\n",
    "    \n",
    "    img = vision_trans(img_aug)\n",
    "    mask = mask_trans(mask_map_aug.get_arr())\n",
    "    \n",
    "    return img, mask\n",
    "\n",
    "val_trans = partial(\n",
    "    trans,\n",
    "    iaa_seq=val_seq,\n",
    "    vision_trans=tensor_trans,\n",
    "    mask_trans=mask_trans\n",
    ")\n",
    "\n",
    "val_dataset = SegmentationDataset(\n",
    "    ROOT_PATH,\n",
    "    training=False,\n",
    "    transform=val_trans\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=NUM_WORKERS,\n",
    "    pin_memory=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model\n",
    "device = torch.device(DEVICE)\n",
    "\n",
    "resnet = resnet101(pretrained=True, num_classes=NUM_CLASSES)\n",
    "\n",
    "if ATTENTION == 'se':\n",
    "    attention = SE_module(resnet.feature_size, R)\n",
    "elif ATTENTION == 'channel':\n",
    "    self.attention = Channel_Attention(resnet.feature_size, R)\n",
    "elif ATTENTION == 'spartial':\n",
    "    attention = Spartial_Attention(K)\n",
    "elif ATTENTION == 'cbam':\n",
    "    attention = nn.Sequential(\n",
    "        Channel_Attention(resnet.feature_size, R),\n",
    "        Spartial_Attention(K)\n",
    "    )\n",
    "elif ATTENTION == 'non_local':\n",
    "    attention = NonLocalBlockND(resnet.feature_size)\n",
    "else:\n",
    "    attention = None\n",
    "    \n",
    "model = SliceNet(resnet, num_classes=NUM_CLASSES, attention=attention)\n",
    "\n",
    "checkpoint_path = os.path.join('./models', '{}_{}'.format(TRAIN_NAME, TRAIN_ID), '{:0>3d}.pth'.format(EPOCH))\n",
    "cp_state_dict = torch.load(checkpoint_path, map_location='cpu')\n",
    "\n",
    "if 'module' in list(cp_state_dict.keys())[0]:\n",
    "    new_state_dict = {}\n",
    "    \n",
    "    for key, value in cp_state_dict.items():\n",
    "        new_state_dict[key.split('.', 1)[1]] = value\n",
    "    \n",
    "    model.load_state_dict(new_state_dict)\n",
    "else:\n",
    "    model.load_state_dict(cp_state_dict)\n",
    "\n",
    "if torch.cuda.device_count() > 1:\n",
    "    model = nn.DataParallel(model)\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tqdm(total=SHOW_COUNT, file=sys.stdout) as pbar:\n",
    "    for frame_no, sample in enumerate(val_dataset):\n",
    "        img_file = os.path.join(\n",
    "            val_dataset.root_path,\n",
    "            'images',\n",
    "            val_dataset.img_filenames[val_dataset.indices[frame_no]]\n",
    "        )\n",
    "        ori_img = cv2.imread(img_file)\n",
    "        ori_img = cv2.cvtColor(ori_img, cv2.COLOR_BGR2RGB)\n",
    "        ori_img = cv2.resize(ori_img, INPUT_SIZE)\n",
    "        \n",
    "        img, mask = sample\n",
    "        \n",
    "        _, h, w = img.shape\n",
    "        \n",
    "        img = img.to(device)\n",
    "        mask = mask.to(device)\n",
    "        \n",
    "        # infer\n",
    "        with torch.no_grad():\n",
    "            logits = model(img.unsqueeze(0))\n",
    "            scores = nn.functional.softmax(logits, 1).squeeze()[-1]\n",
    "            preds = torch.gt(scores, SCORE_THRESHOLD)\n",
    "    \n",
    "            results = torch.ne(preds, torch.gt(mask, 0.5).squeeze())\n",
    "            \n",
    "        pred_img = ori_img.copy()\n",
    "        gt_img = ori_img.copy()\n",
    "        result_img = ori_img.copy()\n",
    "\n",
    "        pred_mask = slice_mask((h, w), MASK_RESIZE_RATIO, preds.squeeze().view(-1).cpu().numpy())\n",
    "        gt_mask = slice_mask((h, w), MASK_RESIZE_RATIO, torch.gt(mask, 0.5).squeeze().view(-1).cpu().numpy())\n",
    "        result_mask = slice_mask((h, w), MASK_RESIZE_RATIO, results.squeeze().view(-1).cpu().numpy())\n",
    "        \n",
    "        pred_img = apply_mask(pred_img, pred_mask, (0, 0, 128))\n",
    "        gt_img = apply_mask(gt_img, gt_mask, (0, 128, 0))\n",
    "        result_img = apply_mask(result_img, result_mask, (128, 0, 0))\n",
    "        \n",
    "        fig=plt.figure(figsize=(16,12))\n",
    "\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.imshow(gt_img)\n",
    "\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.imshow(pred_img)\n",
    "\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.imshow(result_img)\n",
    "        \n",
    "        plt.show()\n",
    "\n",
    "        pbar.update(1)\n",
    "        \n",
    "        if frame_no == SHOW_COUNT-1:\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cc",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
